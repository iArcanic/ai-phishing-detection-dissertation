% ai-phishing-detection-dissertation/report/sections/2-literature-review/challenges-in-ai-based-phishing-detection.tex

\subsection*{Challenges in AI-based phishing detection}

Some simpler models, such as decision trees and random forest, are seen to suffer from a case of overfitting due to the imbalance of datasets and high dimensional data, demonstrated in a study by \cite{harikrishnan2018machine}. A large majority of the reasons as to the performance drop-off is due to dataset limitations like lack of specific features or dataset size, as observed by \cite{ahmad2024across}. They mainly noted how datasets struggled to perform well due to a lack of quality and diverse data. Training times were a significant challenge, not just from dataset size, but the inherent nature of deep learning models (consisting of many layers), that might limit their applicability in real-time situations, as discovered by \cite{kapoor2024comparative}, which is also agreed upon by \cite{atlam2022business}. In \cite{kapoor2024comparative}'s research, they also comment on the constant evolution of sophisticated tactics introduced by attackers, such a deep fakes, new social enginnering tactics, and context-aware attacks -- all with the goal of exploiting human technology. They mention that it is important to factor in an "arms race" between AI models being trained on new data and attackers coming up with new intrusion methods. Traditional models, apart from overfitting, suffer from their sensitivity to parameter turning such as SVM \citep{andriu2023adaptive}. Furthermore, it is a challenge to scale these models given the ever-growing needs of an organisation, as the system must be able to both maintain its performance and optimal detection to deal with increasing workloads, with a lack of real-time implementations and studies, observed by \cite{atlam2022business}. GDPR comes into play here as these systems use user information from email content and metadata, so they need to be ensure compliance and have the appropriate privacy mechanisms implemented. Models are also likely to fall victim of flagging false positives and negatives, and this is a vital aspect in keeping them from being deployed in a real-time setting, emphasised by \cite{vishwanath2011people}. False positives can logically cause disruption and inconvenicnce for businesses and users respectively, where as false negatives are phishing emails that may bypass filters introducing a vulnerability. It is vital for models to prioritise a balance between these two categories of errors for a practical phishing detection system. Another key area is interpretability challenges mentioned by \cite{atlam2022business}, as they state how most AI models are "black-boxed" from users. Only input and output data can be seen, but the process in between are often obscured. The authors drive forward the point of introducing XAI to allow these models to be interpretable, lading to more dependence and instill a sense of confidence in their decisions. There are other studies, such as by \cite{al2024novel}, that note the challenges associated with interpreting feature importance with non-linear models. But the novelty of XAI in general poses several questions which are addressed in the study by \cite{yakandawala2023review}, where several XAI challenges are listed. The most notable being able to strike the perfect balance between explainability and performance. A model that is non-explainable can recieve "public backlash" due to AI errors and biases, lessening trust in their decision making processes. This especially affects deep learning models which face a lot more interpretability issues than other models due to their complexity. A study to complement this, by \cite{reddy2023explainable}, that suggest several more issues concerning interpretability, such a lack of a universal standard or rigid framework in which to develop and evaluate these XAI techniques. The authors claim that is it important to take human factors into account, and it would prove to be effective to understand how users would interact with such explanations. Additionally, there are also ethical issues, as presented by \cite{hanif2021survey}, that cover areas such as accountability, responsibility, transparency, fidelty, bias, casuality, safety, and fairness -- all concerning XAI explanations.
