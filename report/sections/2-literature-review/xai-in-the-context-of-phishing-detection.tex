\*{Explainable AI (XAI) in the context of phishing detection}
There are many studies that put forward the solution of XAI to address interpretability and transparency issues \citep{roshan2022using} associated with systems that employ ML techniques. The goal of XAI here is to serve as a means to understand and inspire confidence in an AI's decision making processes \citep{khanom2025pd_ebm}, from the input all the way through to the output (\citeauthor{jawale2020jeevn}, \citeyear{jawale2020jeevn}; \citeauthor{sanchez2022phishing}, \citeyear{sanchez2022phishing}), with use cases for analysts being able to differentiate between false positives and negatives \citep{van2024applicability}. In particular, there are several existing studies into how XAI plays a role in the context of phishing detection, with a study by \cite{alzahrani2024explainable} proving that both high accuracies (97\%) and reasonable explainability features can be achieved. The importance of such explainable systems is stressed by \cite{shendkar2024enhancing} and further literature comments on how theres been a recent interest for outcome explanations for textual and document based classification tasks (\citeauthor{martens2014explaining}, \citeyear{martens2014explaining}; \citeauthor{lei2016rationalizing}, \citeyear{lei2016rationalizing}). There also has been studies, such as by \cite{vo2024securing}, that comment on how a lack of understandablility and reasoning in these systems can fail to have humans identify anamolies -- useful for phishing emails where its beneficial for the user to act on the system's warning recommendations without posessing full knowledge of the detection mechanisms. One of the main causes of this hypothetically lies within research proposed by \cite{greco2023explaining}, who claim that one of the reasons that many users often fall victim to phishing attacks is the lack if poorly designed interpretability measures, such as dialog boxes for example, without taking into account a user's psychology. The study points out that most of these UI indicators fail to properly explain the rationale behind their decisions. Supported by a psychological study by \cite{anderson2015polymorphic}, suggests how XAI can give insight into a model's decision making processes to address the issue of the inherent black-boxed nature, where warnings should be of a "polymorphic" nature, i.e. warnings adapt based on the threat which users potentially face, meaning XAI systems can either be oriented to be reveal an AI system's inner workings or be focused to solely provide user understandable explanations (\citeauthor{lipton2018mythos}, \citeyear{lipton2018mythos}; \citeauthor{ribeiro2016model}, \citeyear{ribeiro2016model}). There are many ways this can be achieved, and this moves the literature review to specific XAI techniques that can be utilised and practically implemented into AI-powered systems, such as SHAP and LIME as a way to offer insights \citep{shendkar2024enhancing} by providing both global and localised explanations, respectively \citep{palaniappan2020malicious}. There is also an additional higher level explainability technique, EBM, refered to by \cite{hernandes2021phishing}, that is a complete white-boxed approach that aims to construct a predictive model that is already inherently explainable by design and does not require additional interpretability tools post processing \citep{greco2023explaining}. Comparing this with LIME, it builds an interpretation based upon existing black-boxed models' outcomes. Both these techniques provide local and global explanations and are model-agnostic, i.e. they can work with any model or classifier that's ML-based \citep{anderson2015polymorphic}. \cite{greco2023explaining} also dives deeper into the different between local and global explanations, adding onto previous understanding. The author mentiones that local explanations consist of a "feature importance vector" which is a quantifiable value that shows how much that specific feature contributed to the outcome. This is constrating to global explanations, as they require both the entire model and its processing mechanisms. A contextual example of these techniques being implemented is the Phishpedia model presented in research by \cite{lin2021phishpedia}, where it takes URLs and returns an explanation to the user based on the legitimacy of its web domain. The important thing here to note is the attention to user feedback, in the form of dialog boxes as previously mentioned. The alert is very visually appealing and informative to the user, with an emphasis on why the URL is potentially malicious, making compairons of known URLs with proper web domains. Another interpretable approach by \cite{bravo2010bridging} is using a phishing email's metadata and content, showing whether each feature (if any), contributed to its final classification of phishing (or not phishing). A very recent study, \cite{lim2025explicate}, designed an XAI and LLM enhanced phishing detection system called EXPLICATE, utilising LIME, SHAP on a DeepSeek v3 base model with a very practical balance achieved between interpretability, accuracy, and efficiency. There are also specialised, innovative XAI approaches, such as XAIAOA-WPC talked about in research by \cite{alotaibi2025explainable} that boasts a high performance rate of 99.29\%, incorporating mainly LIME for its explainability. There are also systems which employ SHAP, such as the integrated intelligence defender framework, CyberDefender proposed by \cite{krishnaveni2024cyberdefender} that utilised a similar custom XAI solution, called XAI-EFFS, which is a filter feature selection that is ensemble-based. This specific solution takes existing hyper-parameters of a GRU-LSTM deep learning model that used optimisation tactics that are Bayesian-inspired. Feature importance analysis can also be supplemented by XAI, as explored by \cite{fajar2024enhancing}, who discovered that the combination of both RFE and XAI could correctly identify distinct dataset features that largely influenced the model's final decision. In this study, the XGBoost and CatBoost models maintined high accuracy and efficiency, with the former being more scalable and the latter being more robust. To conclude, it is safe to say that there are plenty of integrations of XAI with phishing detection models, often achieving competitive accuracies along with interpretability features. A range of XAI techniques are implemented, the most common being SHAP and LIME, with some uses of EBM. However, there lies some areas and limitations that need to be addressed as emphasised by most studies' future works. It is the aim of this project to address some of those.
