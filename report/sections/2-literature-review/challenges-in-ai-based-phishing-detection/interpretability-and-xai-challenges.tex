% ai-phishing-detection-dissertation/report/sections/2-literature-review/challenges-in-ai-based-phishing-detection/interpretability-and-xai-challenges.tex

\subsubsection*{Interpretability and XAI challenges}
Another key area is interpretability challenges mentioned by \cite{atlam2022business}, as they state how most AI models are "black-boxed" from users. Only input and output data can be seen, but the process in between are often obscured. The authors drive forward the point of introducing XAI to allow these models to be interpretable, lading to more dependence and instill a sense of confidence in their decisions. There are other studies, such as by \cite{al2024novel}, that note the challenges associated with interpreting feature importance with non-linear models. But the novelty of XAI in general poses several questions which are addressed in the study by \cite{yakandawala2023review}, where several XAI challenges are listed. The most notable being able to strike the perfect balance between explainability and performance. A model that is non-explainable can receive "public backlash" due to AI errors and biases, lessening trust in their decision making processes. This especially affects deep learning models which face a lot more interpretability issues than other models due to their complexity. A study to complement this, by \cite{reddy2023explainable}, that suggest several more issues concerning interpretability, such a lack of a universal standard or rigid framework in which to develop and evaluate these XAI techniques. The authors claim that is it important to take human factors into account, and it would prove to be effective to understand how users would interact with such explanations.
