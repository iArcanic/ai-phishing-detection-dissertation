\subsection*{Introduction}

Phishing attacks remain one of the most prominent attack methods faced by cybersecurity, meaning AI-based detection measures become increasingly essential given that attackers are continiously refining their techniques in attempts to bypass traditional detection systems. AI-driven phishing detection systems offer promise, leveraging the power of machine learning (ML) and deep learning models with the task of classifying emails as either legitimate or phishing. It is important to note that interpreting these AI models is still difficult, making it challenging to trust in decisions they make, raising concerns about trust, regulatory compliance, and model robustness in settings such as finance and healthcare, where the decision-making processes must be auditable and explainable \citep{jain2022survey}.\newline

\noindent Explainable AI (XAI) is a means to address these interpretability challenges with AI phishing detection. Techniques like SHAP (SHapley Additive Explanations) and LIME (Local Interpretable Model-agnostic Explanations) can disect the model's decisions, with the aim of helping users understand why a particular email is considered as phishing \citep{lundberg2017unified}. However, research on applying XAI techniques to supplement phishing detection systems is currently limited, with most studies prioritising accuracy rather than interpretability \citep{ribeiro2016model}.\newline

\noindent This literature review therefore explores AI phishing detection techniques being utilised today, their current limitations, and how XAI can be relevant to these systems, with references to existing studies. The review naturally gave rise to some identified research gaps, which lead onto a justification as to why this project was needed. It sets the stage for subsequent the development of an XAI-enhanced phishing detection model.

