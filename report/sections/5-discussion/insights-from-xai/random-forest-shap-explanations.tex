% ai-phishing-detection-dissertation/report/section/5-discussion/xai-insights/random-forest-shap-explanations.tex

\subsubsection*{Random Forest (SHAP) explanations}
SHAP being applied to the Random Forest model presented the following insights:

\begin{itemize}
  \item \textbf{Global feature importance}: The SHAP summary plot highlighted the TF-IDF features that had the most influence on the model's final classification. For example, terms such as "\textit{enron}", "\textit{email}", "\textit{2008}", "\textit{subject}", and "\textit{thanks}", showing a global significance. The plot was vital in visually representing how the presence of terms with high Shapley values either pushed classifications to phishing (positive) or legitimate (negative). A global view was helpful in knowing what general patterns the Random Forest model came to realise for each class.
  \item \textbf{Local instance explanations}: SHAP waterfall plots on the other hand were localised for each individual email instance prediction:
  \begin{itemize}
    \item For email instances correctly predicted as legitimate, the SHAP values show that terms like "\textit{attached}", "\textit{forwarded}", and "\textit{review}" (for Instance 1) contribute the most to this. Professional communication language generally means a lower probability of phishing. This is valid since legitimate emails usually consist of professional language.
    \item For email instances correctly predicted as phishing, the explanations show that some terms like "\textit{sex}", "\textit{things}", as well as nonsensical terms like "fa", "ed", and "es" (for Instances 2 and 3) as key indicators in the model marking an email as phishing. Here however, there is no consistent pattern observed consistent with typical phishing emails, but the presence unusual characters is typical spam content.
    \item The local explanations generated show that the model had a dependency on certain textual components within the TF-IDF features, on which it performed well for on the internal test set. But it was difficult (and more detail was required) on understanding how these TF-IDF features themselves has a contribution to the overall classifications, specifically relating to their technical details (i.e. terms or n-grams) rather than just in a semantic context.
  \end{itemize}
\end{itemize}
