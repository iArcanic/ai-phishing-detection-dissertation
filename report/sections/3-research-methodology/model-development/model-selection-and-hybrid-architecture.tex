% ai-phishing-detection-dissertation/report/sections/3-research-methodology/model-development/model-selection-and-hybrid-architecture.tex

\subsubsection*{Model selection and hybrid architecture}
The architecture will combine the interpretable and high-performance aspects in models that exhibit this well as a means to address the accuracy vs explainability trade-offs identified as part of \hyperref[research-gap-1]{\uline{\textbf{Research Gap 1}}}. A hybrid approach for AI phishing detection systems is motivated by \cite{do2024integrated} and \cite{alzahrani2024explainable}.\newline

\noindent Starting with a baseline model selection:

\begin{itemize}
  \item \textbf{Random forest}:
  \begin{itemize}
    \item Selected for its easy understability through feature importance scores \citep{gupta2021novel}.
    \item Is able to handle a multitude of feature types, like numerical and categorical data, without much preprocessing.
    \item With proper tuning against phishing datasets, it can achieve around 99.57\% accuracy \citep{kapoor2024comparative}.
    \item Is known to struggle with semantic patterns in novel attacks.
  \end{itemize}
  \item \textbf{DistilBERT}:
  \begin{itemize}
    \item A lightweight transformer model variant with 40\% fewer parameters than a typical BERT model \citep{sanchez2022phishing}.
    \item Is able to handle the processing of both text and uRL strings through token-level attention.
    \item Has found to achieve 99.71\% in recent phishing studies \citep{do2024integrated}.
    \item Has the ability to preserve the semantic contexts of phishing texts, specifically when it comes to more niche social engineering attacks.
    \item Is quite black-boxed in nature, so it complicates explainability.
  \end{itemize}
\end{itemize}

\noindent Now concerning the hybrid model fusion strategy, where models are integrated together via a weighted probability ensemble:

\begin{itemize}
  \item \textbf{Weight allocation}:
  \begin{itemize}
    \item \textit{Random forest}: 40\% weight, with a priority for interpretability in a a security-context.
    \item \textit{DistilBERT}: 60\% weight, utilising its well-performing capabilities to represent text for more complex linguistic patterns.
    \item Ratio is in agreement with \cite{alzahrani2024explainable}'s findings for the best balance in hybrid weighting.
  \end{itemize}
  \item \textbf{Fusion mechanism}:
  \begin{equation}
    P_{final}(phishing) = 0.4 \cdot P_{RF} + 0.6 \cdot P_{BERT}
  \end{equation}
  Where \( P \) represents the model's phishing probaility output.
  \item \textbf{Fallback mechanism}:
  \begin{itemize}
    \item If there is a slight disagreement between models, then default to the prediction made by DistilBERT -- reduces false negatives.
    \item It is better have false alarms (false positives) than missed phishing attacks (false negatives) in critical security contexts.
  \end{itemize}
  \item \textbf{XAI compatability}:
  \begin{itemize}
    \item SHAP can explain decisions made by random forest via feature importance.
    \item LIME can generate local explanations for DistilBERT's outcomnes.
    \item A unified interface is able to present both types of explanations -- in a user-friendly way.
  \end{itemize}
\end{itemize}

\noindent There are some key theoretical benefits in adopting the defined approach:

\begin{itemize}
  \item \hyperref[research-gap-1]{\uline{\textbf{Research Gap 1}}} is directly addressed, by balancing both the performance and of DistilBERT and accuracy of random forest.
  \item Ensures compliance with GDPR's "right to explanation" via XAI support.
  \item Is scalable to new attack types through DistilBERT's learning capabilities.
\end{itemize}
