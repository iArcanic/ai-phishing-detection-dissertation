 % ai-phishing-detection-dissertation/report/sections/3-research-methodology/model-development/training-and-optimisation.tex

\subsubsection*{Training and optimisation}
The training pipeline for this implementation is particulatly designed with respect to optimisation of detection performance as well as the quality of explanation output to meet \hyperref[research-gap-1]{\uline{\textbf{Research Gap 1}}}. The methods described below are adapted from \cite{do2024integrated} and \cite{shendkar2024enhancing}, in the context of phishing email detection.\newline

\noindent The training will adopt these following phases:

\begin{enumerate}
  \item \textbf{Phase 1: Pretraining}
  \begin{itemize}
    \item For DistilBERT, initialise weights from HuggingFace's prebuilt "\texttt{distilbert-base-uncased}" mode, which is then fine-tuned upon:
    \begin{itemize}
      \item General phishing texts from PhishTank URLs and Enron phishing emails.
      \item Set the task to a binary classification task, where the states are phishing or legitimate emails.
      \item Set the hyparameters to: 3 epochs, \texttt{lr = 2e-5}, and \texttt{batch = 32}.
    \end{itemize}
    \item For random forest, train on structured features:
    \begin{itemize}
      \item Structured features include URL length and header mismatches.
      \item For class weights, use 70\% for phishing emails and 30\% for legitimate emails.
    \end{itemize}
  \end{itemize}
  \item \textbf{Phase 2: Hybrid Fine-tuning}
  \begin{itemize}
    \item Lower layers of DistilBERT should be frozen, and update only the attention heads.
    \item Use a joint optimisation that involves both models, with a custom loss algorithm:
    \begin{equation}
      \mathcal{L} = 0.6\mathcal{L}_{F_\beta} + 0.4\mathcal{L}_{expl}
    \end{equation}
    where $\mathcal{L}_{F_\beta}$ is the F2 score loss ($\beta=2$) and $\mathcal{L}_{expl}$ punishes SHAP/LIME disagreements.
    \item Utilise early stopping if validation F2-scores dip for 4-5 epochs.
  \end{itemize}
\end{enumerate}

\noindent Hyperparameters for both models should be optimised.

\begin{itemize}
  \item \textbf{Bayesian search}: Conduct around 50 trials per model using \texttt{optuna}, a hyperparameter optimisation framework \citep{optuna2025}. 
  \begin{itemize}
    \item For random forest, set the \texttt{max\_depth} from $3 - 20$ and texttt{n\_estimators} from $50 - 500$.
    \item For DistilBERT, set the learning rate from $1e - 6$ to $5e - 5$, and dropout rate from $0.1 - 0.4$.
  \item \textbf{XAI-guided pruning}: Removes features that rank on SHAP importance of less than $0.01$, helping to reduce unecessary noise.
  \item \textbf{Limitations}: Maintains a $\geq$95\% of the original F2-score during pruning, so there is an unavoidable 5\% loss.
  \end{itemize}
\end{itemize}
