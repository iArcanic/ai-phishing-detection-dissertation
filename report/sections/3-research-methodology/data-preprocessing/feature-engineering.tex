% ai-phishing-detection-dissertation/report/sections/3-research-methodology/data-preprocessing/feature-engineering.tex

\subsubsection*{Feature engineering}
Efficient, purposeful, and thorough feature engineering is important to differentiate between legitimate emails and phishing content. This section takes inspiration from \citep{do2024integrated} and \citep{hamid2013using} with the aim of creating three feature classes -- perfected for model performance and subsequent explainability.\newline

\noindent For the Random Forest model, numerical features were generated from the textual content, and this was via TF-IDF techniques. A "\texttt{TfidfVectorizer}" model from the "\texttt{scikit-learn}" was utilised, with the cleaned email bodies and subject lines concatenated, and fed into the input to be vectorised. Parameters for the vectoriser include: "\texttt{stop\_words='english'}" to exclude common English words from the feature set, "\texttt{max\_df=0.90}" to ignore very common terms in more than 90\% of documents, "\texttt{min\_df=5}" to ignore very rare terms in less than 5\% of documents, "\texttt{ngram\_range=(1, 2)}" to consider unigrams and bigrams, "\texttt{max\_features=15000}" to limit vocabulary size, and "\texttt{sublinear\_tf=True}" to apply sublinear scaling.\newline

\noindent For the DistilBERT model, since it's transformer-based, it's capable of directly processing text via a specialised tokeniser, i.e. the "\texttt{DistilBertTokenizerFast}". Input of the cleaned text (email subject and body combined) was fed into the tokeniser, and this was then subsequently converted into the necessary format, i.e. input IDs and attention masks, for the model. The model takes this information and generates further contextual embeddings. No additional feature engineering, i.e. with TF-IDF for Random Forest, was required, as the model's architecture is designed to handle the text data directly.
