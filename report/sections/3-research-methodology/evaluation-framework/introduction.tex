% ai-phishing-detection-dissertation/report/sections/3-research-methodology/evaluation-framework/introduction.tex

The following section presents a rigorous assessment methodology to evaluate the XAI phishing detection system under. It validates both the technical performance and operational usability, with respect to human factors, addressing \uline{\textbf{Research gaps} \hyperref[research-gap-1]{\uline{\textbf{1}}}, \hyperref[research-gap-2]{\textbf{2}}, and \hyperref[research-gap-3]{\textbf{3}}}. It builds on methodologies and principles from \cite{reddy2023explainable} and \cite{van2024applicability}, to evaluate:

\begin{itemize}
  \item \textbf{Model effectiveness}: Evaluated via traditional metrics such as F2-score, precision, recall, and accuracy, as well as phishing benchmarks like adversarial robustness and cost-sensitive accuracy. These metrics are crucial for assessing the model's performance in identifying phishing emails.
  \item \textbf{Explainability quality}: Computational explanation metrics like SHAP fidelity and LIME consistency, as well as human-factor studies for expert trust and speed of decisions.
  \item \textbf{Benchmarking}: Uses a combination of commercial and academic solutions to compare the system against.
\end{itemize}

\noindent The framework is set to following the steps of the "\textit{Deployment Challenges}" identified as part of research conducted by \citep{atlam2022business}. Not only does this framework test the algorithmic and human-centric performance of the system, but also the practical constraints if deployed in a real-world setting, like integration with a security analyst's workflow and compliance with GDPR. As a result of this thorough framework, it feeds directly into a measurement of how ready the system is for production environments, but also serves as a basis in which to understand the benefits and costs of XAI, for such a context.
