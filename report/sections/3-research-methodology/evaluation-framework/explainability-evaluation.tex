% ai-phishing-detection-dissertation/report/sections/3-research-methodology/evaluation-framework/explainability-evaluation.tex

\subsubsection*{Explainability evaluation}
The aspect of explanability needs to be evaluated via quantitative and human-centric methods. It follows frameworks from \citep{reddy2023explainable} and \citep{van2024applicability}, with metrics addressing \hyperref[research-gap-3]{\uline{\textbf{Research Gap 3}}}, i.e. a need for a standardised XAI evaluation framework.\newline

\noindent Quantitative metrics are first used to numerically analyse the effectiveness of explanation completeness, cardinality rules, and perturbation metrics.

\begin{itemize}
  \item \textbf{Explanation fidelity}:
  \begin{itemize}
    \item A measurement of agreement between SHAP/LIME predictions and explanations.
    \begin{equation}
      \text{Fidelity} = \frac{1}{N}\sum_{i=1}^N \mathbb{I}(\text{sign}(E_i) = \text{sign}(P_i - 0.5))
    \end{equation}
    , where \(E_i\) is the explanation strength and \(P_i\) is the prediction for the \(i^{th}\) instance.
  \item Target should be \(\geq\)90\%, as per \cite{shendkar2024enhancing}.
  \end{itemize}
  \item \textbf{Feature importance consistency}:
  \begin{itemize}
    \item Rank correlation (Kendall's "\(\tau\)" or Spearman's "\(\rho\)") between:
    \begin{itemize}
      \item SHAP values and ground-truth feature importance phishing indicators from \cite{greco2023explaining}.
      \item Apply LIME explanations across similar instances.
    \end{itemize}
    \item Set a threshold of "\(\geq\)0.7" for Kendall's "\(\tau\)" and "\(\geq\)0.8" for Spearman's "\(\rho\)", for critical features.
  \end{itemize}
  \item \textbf{Explanation stability}:
  \begin{itemize}
    \item A measurement of the robustness of explanations to perturbations in input data:
    \begin{equation}
      S = 1 - \frac{||\text{SHAP}(x) - \text{SHAP}(x + \epsilon)||_2}{||\text{SHAP}(x)||_2}
    \end{equation}
    \item Requires \(S \geq 0.85\) for adversarial robustness, as per \cite{atlam2022business}.
  \end{itemize}
\end{itemize}

\noindent Human-centric evaluation involves the input of human users to assess the effectiveness of the XAI model. This is done via expert surveys, decision time measurements, and error analysis. The goal is to measure the impact of XAI on user decision-making and trust in the model's predictions.

\begin{itemize}
  \item \textbf{Expert surveys (Likert Scale)}:
  \begin{itemize}
    \item Use experts in the field to assess:
    \begin{itemize}
      \item \textit{Explanation usefulness}: "\textit{The explanation helped me judge the risk of the phishing email}", complemented by a ranking scale from 1-5.
      \item \textit{Actionability}: "\textit{I was able to determine the subsequent action to take based on the explanation}", complemented by a ranking scale from 1-5.
    \end{itemize}
    \item Set a threshold of for "\(\geq\)4" mean score for both metrics, regarding their critical features.
  \end{itemize}
  \item \textbf{Decision time measurement}:
  \begin{itemize}
    \item A record of the time reduction when users use XAI, as opposed to raw decision outputs.
    \item A baseline criteria to compare: 2.1 minutes per minute alert without XAI \citep{shirazi2022towards}.
  \end{itemize}
  \item \textbf{Error analysis}:
  \begin{itemize}
    \item Track any false positives and false negatives in the XAI model, especially if users were misled.
    \item Modes of failure should be classified as such \citep{lipton2018mythos}:
    \begin{itemize}
      \item Over-trusting in more fraudulent features.
      \item Under weighting critical indicators.
    \end{itemize}
  \end{itemize}
\end{itemize}
