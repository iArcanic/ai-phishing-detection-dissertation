\section{Literature review}

\subsection*{Introduction}
Phishing attacks remain one of the most prominent attack methods faced by cybersecurity, meaning AI-based detection measures become increasingly essential given that attackers are continiously refining their techniques in attempts to bypass traditional detection systems. AI-driven phishing detection systems offer promise, leveraging the power of machine learning (ML) and deep learning models with the task of classifying emails as either legitimate or phishing. It is important to note that interpreting these AI models is still difficult, making it challenging to trust in decisions they make, raising concerns about trust, regulatory compliance, and model robustness in settings such as finance and healthcare, where the decision-making processes must be auditable and explainable.

\noindent Explainable AI (XAI) is a means to address these interpretability challenges with AI phishing detection. Techniques like SHAP (SHapley Additive Explanations) and LIME (Local Interpretable Model-agnostic Explanations) can disect the model's decisions, with the aim of helping users understand why a particular email is considered as phishing. However, research on applying XAI techniques to supplement phishing detection systems is currently limited, with most studies prioritising accuracy rather than interpretability.

\noindent This literature review therefore explores any existing AI phishing detection techniques, their current limitations, and how XAI can be implemented to improve transparency and confidence in these systems, suitable for a real-time environment. In addition, key research gaps are identified, such as the trade-offs between model accuracy and interpretability. It sets the stage for subsequent the development of an XAI-enhanced phishing detection model.

\subsection*{AI-based phishing detection approaches}

\subsubsection*{Traditional machine learning approaches}

\subsubsection*{Deep learning and NLP approaches}

\subsubsection*{Hybrid models and ensemble learning}

\subsection*{Limitations of current AI-based approaches}

\subsubsection*{Black-box nature of AI models}

\subsubsection*{Accuracy vs. explainability trade-off}

\subsubsection*{False positives and false negatives}

\subsubsection*{Dataset challenges}

\subsection*{Introduction to Explainable AI (XAI)}

\subsubsection*{What is Explainable AI?}

\subsubsection*{Why is XAI important in phishing detection?}

\subsection*{XAI techniques in phishing detection}

\subsubsection*{Model-specific vs. model-agnostic XAI}

\subsubsection*{SHAP (SHapley Additive Explanations)}

\subsubsection*{LIME (Local Interpretable Model-Agnostic Explanations)}

\subsubsection*{Other XAI approaches}

\subsection*{Identified research gaps}

\subsubsection*{Lack of comparative studies}

\subsubsection*{Interpretability vs. performance trade-off}

\subsubsection*{Real-world deployment challenges}

\subsection*{Project's contribution in addressing research gaps}

\subsubsection*{Developing a transparent phishing model}

\subsubsection*{Comparing SHAP and LIME for phishing email classification}

\subsubsection*{Ensuring real-world applicability}
