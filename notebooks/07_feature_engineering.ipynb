{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "LznhluwWb9sR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Import libraries"
      ],
      "metadata": {
        "id": "T0mBSBv_cNof"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsTbzSUIbwyh",
        "outputId": "6710edba-62e2-4678-e5b3-b7b27e3eb8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading libraries...\n",
            "Loading imported...\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading libraries...\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gdown\n",
        "\n",
        "# For saving the TfidfVectorizer model\n",
        "import joblib\n",
        "\n",
        "# To save sparse matrices if you use them\n",
        "from scipy.sparse import save_npz, load_npz\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "\n",
        "print(\"Loading imported...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Load split datasets\n",
        "\n",
        "Load the `train_corpus.csv`, `validation_corpus.csv`, and `test_corpus.csv` files that were generated in the previous notebook."
      ],
      "metadata": {
        "id": "WoSlpOA0ihFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_CORPUS_GDRIVE_FILE_ID = '1x1E4c6lK7H888RC2ucyhRZuSEXn23x0Z'\n",
        "TRAIN_CORPUS_LOCAL_FILENAME = 'train_corpus.csv'\n",
        "\n",
        "VALIDATION_CORPUS_GDRIVE_FILE_ID = '19jIrYmW5MkY4D4AApDlA_Bu2QcgC5HcP'\n",
        "VALIDATION_CORPUS_LOCAL_FILENAME = 'validation_corpus.csv'\n",
        "\n",
        "TEST_CORPUS_GDRIVE_FILE_ID = '1bR5kipQPB21VSvjpLypbzkQAP0v-sAaw'\n",
        "TEST_CORPUS_LOCAL_FILENAME = 'test_corpus.csv'\n",
        "\n",
        "# --- Function to download with gdown if file doesn't exist ---\n",
        "# (This function can be reused from your previous notebooks if you put it in a utils script)\n",
        "def download_file_from_gdrive(file_id, local_filename):\n",
        "    if not os.path.exists(local_filename):\n",
        "        print(f\"Downloading {local_filename} from Google Drive...\")\n",
        "        gdrive_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "        try:\n",
        "            # import gdown # Ensure gdown is imported (usually in Section 1)\n",
        "            gdown.download(gdrive_url, local_filename, quiet=False)\n",
        "            print(f\"{local_filename} downloaded successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR downloading {local_filename}: {e}. Check File ID and share settings.\")\n",
        "            return False\n",
        "\n",
        "    else:\n",
        "        print(f\"{local_filename} already exists in Colab runtime.\")\n",
        "\n",
        "    return os.path.exists(local_filename)\n",
        "\n",
        "# --- Initialize DataFrames ---\n",
        "df_train = pd.DataFrame()\n",
        "df_val = pd.DataFrame()\n",
        "df_test = pd.DataFrame()\n",
        "all_files_loaded_successfully = True\n",
        "\n",
        "# --- Load Training Data ---\n",
        "if download_file_from_gdrive(TRAIN_CORPUS_GDRIVE_FILE_ID, TRAIN_CORPUS_LOCAL_FILENAME):\n",
        "    try:\n",
        "        df_train = pd.read_csv(TRAIN_CORPUS_LOCAL_FILENAME)\n",
        "        print(f\"Loaded {TRAIN_CORPUS_LOCAL_FILENAME}, shape: {df_train.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {TRAIN_CORPUS_LOCAL_FILENAME}: {e}\")\n",
        "        all_files_loaded_successfully = False\n",
        "\n",
        "else:\n",
        "    all_files_loaded_successfully = False\n",
        "\n",
        "# --- Load Validation Data ---\n",
        "if download_file_from_gdrive(VALIDATION_CORPUS_GDRIVE_FILE_ID, VALIDATION_CORPUS_LOCAL_FILENAME):\n",
        "    try:\n",
        "        df_val = pd.read_csv(VALIDATION_CORPUS_LOCAL_FILENAME)\n",
        "        print(f\"Loaded {VALIDATION_CORPUS_LOCAL_FILENAME}, shape: {df_val.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {VALIDATION_CORPUS_LOCAL_FILENAME}: {e}\")\n",
        "        all_files_loaded_successfully = False\n",
        "\n",
        "else:\n",
        "    all_files_loaded_successfully = False\n",
        "\n",
        "# --- Load Test Data ---\n",
        "if download_file_from_gdrive(TEST_CORPUS_GDRIVE_FILE_ID, TEST_CORPUS_LOCAL_FILENAME):\n",
        "    try:\n",
        "        df_test = pd.read_csv(TEST_CORPUS_LOCAL_FILENAME)\n",
        "        print(f\"Loaded {TEST_CORPUS_LOCAL_FILENAME}, shape: {df_test.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {TEST_CORPUS_LOCAL_FILENAME}: {e}\")\n",
        "        all_files_loaded_successfully = False\n",
        "\n",
        "else:\n",
        "    all_files_loaded_successfully = False\n",
        "\n",
        "# Check if train at least is loaded\n",
        "if all_files_loaded_successfully and not df_train.empty:\n",
        "    print(\"\\nTraining data head:\")\n",
        "    print(df_train.head(2))\n",
        "    print(f\"Training data label distribution:\\n{df_train['label'].value_counts(normalize=True)}\")\n",
        "\n",
        "elif not all_files_loaded_successfully:\n",
        "    print(\"\\nOne or more dataset files (train, val, test) could not be downloaded or loaded. Please check GDrive IDs and share settings.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKN1-EupqDLk",
        "outputId": "b3ba8c6d-dcaf-4cd9-868a-96f6c5b2a52c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading train_corpus.csv from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1x1E4c6lK7H888RC2ucyhRZuSEXn23x0Z\n",
            "To: /content/train_corpus.csv\n",
            "100%|██████████| 29.4M/29.4M [00:00<00:00, 46.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_corpus.csv downloaded successfully.\n",
            "Loaded train_corpus.csv, shape: (29280, 3)\n",
            "Downloading validation_corpus.csv from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19jIrYmW5MkY4D4AApDlA_Bu2QcgC5HcP\n",
            "To: /content/validation_corpus.csv\n",
            "100%|██████████| 6.29M/6.29M [00:00<00:00, 27.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation_corpus.csv downloaded successfully.\n",
            "Loaded validation_corpus.csv, shape: (6274, 3)\n",
            "Downloading test_corpus.csv from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bR5kipQPB21VSvjpLypbzkQAP0v-sAaw\n",
            "To: /content/test_corpus.csv\n",
            "100%|██████████| 6.81M/6.81M [00:00<00:00, 22.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_corpus.csv downloaded successfully.\n",
            "Loaded test_corpus.csv, shape: (6275, 3)\n",
            "\n",
            "Training data head:\n",
            "                                                                                                                                            body_cleaned  \\\n",
            "0  start date 12702 hourahead hour 5 no ancillary schedules awarded. no variances detected. log messages parsing file  oportlandwestdeskcalifornia sc...   \n",
            "1                                                                                                                the world's largest online health shop.   \n",
            "\n",
            "                     subject_cleaned  label  \n",
            "0  start date 12702 hourahead hour 5      0  \n",
            "1          5 meds every woman needs.      1  \n",
            "Training data label distribution:\n",
            "label\n",
            "1    0.521858\n",
            "0    0.478142\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Prepare text for vectorisation\n",
        "\n",
        "The loaded `df_train`, `df_val`, and `df_test` DataFrames contain 'body_cleaned' and potentially 'subject_cleaned' columns.\n",
        "We will:\n",
        "\n",
        "1. Fill any NaN values in these text columns with empty strings.\n",
        "2. Create a combined text feature (e.g., 'subject_cleaned' + 'body_cleaned') if 'subject_cleaned' is available and desired for feature extraction. Otherwise, we'll use 'body_cleaned'.\n",
        "This combined (or selected) text will be used for TF-IDF vectorization."
      ],
      "metadata": {
        "id": "dY_PzNayxphh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will be set to the name of the column to vectorise\n",
        "FEATURE_TEXT_COLUMN = None\n",
        "\n",
        "# Ensure dataframes are loaded (basic check)\n",
        "if 'df_train' not in globals() or df_train.empty or \\\n",
        "   'df_val' not in globals() or df_val.empty or \\\n",
        "   'df_test' not in globals() or df_test.empty:\n",
        "    print(\"ERROR: df_train, df_val, or df_test is not loaded or is empty. Please run Section 2 successfully.\")\n",
        "\n",
        "else:\n",
        "    TEXT_FEATURE_COL_BODY = 'body_cleaned'\n",
        "\n",
        "    # This was the name used when creating the splits\n",
        "    TEXT_FEATURE_COL_SUBJECT = 'subject_cleaned'\n",
        "\n",
        "    # Fill NaN values in text columns with empty strings\n",
        "    for df_name, df in [('df_train', df_train), ('df_val', df_val), ('df_test', df_test)]:\n",
        "        if TEXT_FEATURE_COL_BODY in df.columns:\n",
        "            df[TEXT_FEATURE_COL_BODY] = df[TEXT_FEATURE_COL_BODY].fillna('')\n",
        "\n",
        "        else:\n",
        "            print(f\"Warning: Column '{TEXT_FEATURE_COL_BODY}' not found in {df_name}.\")\n",
        "            # If critical, you might want to stop or handle this error more robustly\n",
        "\n",
        "        if TEXT_FEATURE_COL_SUBJECT in df.columns:\n",
        "            df[TEXT_FEATURE_COL_SUBJECT] = df[TEXT_FEATURE_COL_SUBJECT].fillna('')\n",
        "        # No warning if subject is missing, as body is primary\n",
        "\n",
        "    # Strategy for combining text features (concatenate subject and body)\n",
        "    # This was defined when creating the split DataFrames. If 'subject_cleaned' was included in X for splitting, it's here.\n",
        "\n",
        "    def combine_text_features_for_vectorization(row):\n",
        "        body = str(row.get(TEXT_FEATURE_COL_BODY, \"\"))\n",
        "        subject = str(row.get(TEXT_FEATURE_COL_SUBJECT, \"\"))\n",
        "\n",
        "        # Subject first\n",
        "        return subject + \" \" + body\n",
        "\n",
        "    # Check if subject column exists to decide on the feature column\n",
        "    if TEXT_FEATURE_COL_SUBJECT in df_train.columns:\n",
        "        print(\"Creating 'text_features_combined' by concatenating subject and body...\")\n",
        "        df_train['text_features_combined'] = df_train.apply(combine_text_features_for_vectorization, axis=1)\n",
        "        df_val['text_features_combined'] = df_val.apply(combine_text_features_for_vectorization, axis=1)\n",
        "        df_test['text_features_combined'] = df_test.apply(combine_text_features_for_vectorization, axis=1)\n",
        "        FEATURE_TEXT_COLUMN = 'text_features_combined'\n",
        "\n",
        "    elif TEXT_FEATURE_COL_BODY in df_train.columns:\n",
        "        print(f\"Using only '{TEXT_FEATURE_COL_BODY}' for feature engineering as subject column was not found or not consistently available.\")\n",
        "        FEATURE_TEXT_COLUMN = TEXT_FEATURE_COL_BODY\n",
        "\n",
        "    else:\n",
        "        print(f\"ERROR: Critical text column '{TEXT_FEATURE_COL_BODY}' is missing. Cannot proceed with feature engineering.\")\n",
        "        # This state should ideally be caught earlier if df_train is empty or columns are wrong\n",
        "\n",
        "    if FEATURE_TEXT_COLUMN and FEATURE_TEXT_COLUMN in df_train.columns:\n",
        "        print(f\"Final column to be used for TF-IDF: '{FEATURE_TEXT_COLUMN}'\")\n",
        "        print(\"Sample of text to be vectorized (from training data, first 2 entries):\")\n",
        "        print(df_train[FEATURE_TEXT_COLUMN].head(2).tolist())\n",
        "\n",
        "    elif FEATURE_TEXT_COLUMN is None:\n",
        "         print(\"Feature text column could not be determined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddn278hQa63V",
        "outputId": "268a07e5-9bf0-47a0-ab65-81e51ab50b33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating 'text_features_combined' by concatenating subject and body...\n",
            "Final column to be used for TF-IDF: 'text_features_combined'\n",
            "Sample of text to be vectorized (from training data, first 2 entries):\n",
            "['start date 12702 hourahead hour 5 start date 12702 hourahead hour 5 no ancillary schedules awarded. no variances detected. log messages parsing file  oportlandwestdeskcalifornia schedulingiso final schedules2002012705.txt', \"5 meds every woman needs. the world's largest online health shop.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 TF-IDF Vectorisation\n",
        "\n",
        "Convert the prepared text data (`FEATURE_TEXT_COLUMN`) into numerical features using Term Frequency-Inverse Document Frequency (TF-IDF).\n",
        "The TF-IDF vectorizer will be **fitted ONLY on the training data** (`df_train[FEATURE_TEXT_COLUMN]`).\n",
        "Then, it will be used to **transform** the training, validation, and test sets.\n",
        "The fitted vectorizer will also be saved for later use."
      ],
      "metadata": {
        "id": "mxY5DofyW96c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf = None\n",
        "X_val_tfidf = None\n",
        "X_test_tfidf = None\n",
        "\n",
        "# Renamed to avoid conflict with sklearn's TfidfVectorizer class\n",
        "tfidf_vectorizer_model = None\n",
        "\n",
        "if FEATURE_TEXT_COLUMN and \\\n",
        "   'df_train' in globals() and not df_train.empty and FEATURE_TEXT_COLUMN in df_train.columns and \\\n",
        "   'df_val' in globals() and not df_val.empty and FEATURE_TEXT_COLUMN in df_val.columns and \\\n",
        "   'df_test' in globals() and not df_test.empty and FEATURE_TEXT_COLUMN in df_test.columns:\n",
        "\n",
        "    # Initialize TF-IDF Vectorizer\n",
        "    tfidf_vectorizer_model = TfidfVectorizer(\n",
        "        stop_words='english',\n",
        "        max_df=0.90,        # Ignore terms that appear in > 90% of documents (too common)\n",
        "        min_df=5,           # Ignore terms that appear in < 5 documents (too rare)\n",
        "        ngram_range=(1, 2), # Consider unigrams and bigrams\n",
        "        max_features=15000, # Limit vocabulary size (adjust based on performance/memory)\n",
        "        sublinear_tf=True   # Apply sublinear tf scaling (i.e., replace tf with 1 + log(tf))\n",
        "    )\n",
        "\n",
        "    print(\"Fitting TF-IDF Vectorizer on training data...\")\n",
        "    start_time_tfidf_fit = time.time()\n",
        "    X_train_tfidf = tfidf_vectorizer_model.fit_transform(df_train[FEATURE_TEXT_COLUMN])\n",
        "    fit_duration = time.time() - start_time_tfidf_fit\n",
        "    print(f\"TF-IDF fitting completed in {fit_duration:.2f} seconds.\")\n",
        "\n",
        "    print(\"Transforming validation data...\")\n",
        "    start_time_tfidf_transform_val = time.time()\n",
        "    X_val_tfidf = tfidf_vectorizer_model.transform(df_val[FEATURE_TEXT_COLUMN])\n",
        "    transform_val_duration = time.time() - start_time_tfidf_transform_val\n",
        "    print(f\"Validation data transformation completed in {transform_val_duration:.2f} seconds.\")\n",
        "\n",
        "    print(\"Transforming test data...\")\n",
        "    start_time_tfidf_transform_test = time.time()\n",
        "    X_test_tfidf = tfidf_vectorizer_model.transform(df_test[FEATURE_TEXT_COLUMN])\n",
        "    transform_test_duration = time.time() - start_time_tfidf_transform_test\n",
        "    print(f\"Test data transformation completed in {transform_test_duration:.2f} seconds.\")\n",
        "\n",
        "    print(f\"\\nShape of TF-IDF features for training data: {X_train_tfidf.shape}\")\n",
        "    print(f\"Shape of TF-IDF features for validation data: {X_val_tfidf.shape}\")\n",
        "    print(f\"Shape of TF-IDF features for test data: {X_test_tfidf.shape}\")\n",
        "\n",
        "    # Save the fitted TF-IDF vectorizer\n",
        "    try:\n",
        "        joblib.dump(tfidf_vectorizer_model, 'tfidf_vectorizer.joblib')\n",
        "        print(\"TF-IDF vectorizer saved to '/content/tfidf_vectorizer.joblib'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving TF-IDF vectorizer: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"Cannot perform TF-IDF: Training data/validation/test data or the designated FEATURE_TEXT_COLUMN is missing or empty.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xgnfawba7wi",
        "outputId": "61606d2a-6fe6-4e7e-f7dd-aecc440588de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TF-IDF Vectorizer on training data...\n",
            "TF-IDF fitting completed in 12.80 seconds.\n",
            "Transforming validation data...\n",
            "Validation data transformation completed in 1.24 seconds.\n",
            "Transforming test data...\n",
            "Test data transformation completed in 1.37 seconds.\n",
            "\n",
            "Shape of TF-IDF features for training data: (29280, 15000)\n",
            "Shape of TF-IDF features for validation data: (6274, 15000)\n",
            "Shape of TF-IDF features for test data: (6275, 15000)\n",
            "TF-IDF vectorizer saved to '/content/tfidf_vectorizer.joblib'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Prepare Labels and Handle Class Imbalance with SMOTE (Optional)\n",
        "\n",
        "Extract labels (`y_train`, `y_val`, `y_test`).\n",
        "Your combined dataset should be quite balanced (approx 40k ham, 40k phishing).\n",
        "SMOTE is likely **not needed for class balancing** itself in this case.\n",
        "It could *potentially* be used for data augmentation of the phishing class if desired, but for now, we will assume it's not applied due to the balanced nature.\n",
        "**SMOTE is applied ONLY to the training features (`X_train_tfidf`) and training labels (`y_train`).**"
      ],
      "metadata": {
        "id": "sfRw1v8jZJgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = pd.Series(dtype='int')\n",
        "y_val = pd.Series(dtype='int')\n",
        "y_test = pd.Series(dtype='int')\n",
        "\n",
        "# Initialise final training features, might be updated by SMOTE\n",
        "X_train_final = X_train_tfidf\n",
        "\n",
        "# Initialise final training labels\n",
        "y_train_final = pd.Series(dtype='int')\n",
        "\n",
        "if 'df_train' in globals() and not df_train.empty and 'df_val' in globals() and not df_val.empty and 'df_test' in globals() and not df_test.empty and X_train_tfidf is not None:\n",
        "    y_train = df_train['label'].astype(int)\n",
        "    y_val = df_val['label'].astype(int)\n",
        "    y_test = df_test['label'].astype(int)\n",
        "\n",
        "    # Initialise before potential SMOTE\n",
        "    y_train_final = y_train.copy()\n",
        "\n",
        "    print(\"Original training label distribution:\")\n",
        "    print(y_train.value_counts(normalize=True))\n",
        "    print(f\"Original training feature shape: {X_train_tfidf.shape}\")\n",
        "\n",
        "    # --- Optional: Apply SMOTE to the training data ---\n",
        "    # Given your dataset is already balanced (approx. 40k ham, 40k phish),\n",
        "    # SMOTE for *balancing* is not needed.\n",
        "    # You might consider it for *augmentation* if you wanted more synthetic phishing samples,\n",
        "    # but for now, let's keep it simple and set APPLY_SMOTE to False.\n",
        "\n",
        "    # Set to True only if you have a specific reason for augmentation\n",
        "    APPLY_SMOTE = False\n",
        "\n",
        "    if APPLY_SMOTE:\n",
        "        print(\"\\nApplying SMOTE to training data (for augmentation)...\")\n",
        "        try:\n",
        "            smote = SMOTE(random_state=42)\n",
        "\n",
        "            # Note: SMOTE expects numerical features. X_train_tfidf is already numerical (sparse matrix).\n",
        "            X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "            X_train_final = X_train_smote\n",
        "\n",
        "            # Convert numpy array from SMOTE back to pandas Series\n",
        "            y_train_final = pd.Series(y_train_smote)\n",
        "\n",
        "            print(\"SMOTE applied. New training label distribution:\")\n",
        "            print(y_train_final.value_counts(normalize=True))\n",
        "            print(f\"Shape of training features after SMOTE: {X_train_final.shape}\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"imblearn.over_sampling.SMOTE not found. Skipping SMOTE. Install with: !pip install imbalanced-learn\")\n",
        "\n",
        "            # Fallback to original\n",
        "            X_train_final = X_train_tfidf\n",
        "\n",
        "            # Fallback to original\n",
        "            y_train_final = y_train\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during SMOTE: {e}. Using original training data.\")\n",
        "\n",
        "            # Fallback\n",
        "            X_train_final = X_train_tfidf\n",
        "\n",
        "            # Fallback\n",
        "            y_train_final = y_train\n",
        "\n",
        "    else:\n",
        "        print(\"SMOTE not applied (dataset is balanced or APPLY_SMOTE=False). Using original training data for X_train_final, y_train_final.\")\n",
        "        X_train_final = X_train_tfidf\n",
        "\n",
        "        # Already a pandas Series\n",
        "        y_train_final = y_train\n",
        "\n",
        "else:\n",
        "    print(\"Cannot prepare labels or apply SMOTE: Prerequisite data (df_train, X_train_tfidf, etc.) not available or empty.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKislphBa8tC",
        "outputId": "9b9be555-e39b-4616-a125-2cb5bd08b249"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training label distribution:\n",
            "label\n",
            "1    0.521858\n",
            "0    0.478142\n",
            "Name: proportion, dtype: float64\n",
            "Original training feature shape: (29280, 15000)\n",
            "SMOTE not applied (dataset is balanced or APPLY_SMOTE=False). Using original training data for X_train_final, y_train_final.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Save engineered features and labels\n",
        "\n",
        "Save the TF-IDF feature matrices (which are sparse) and the corresponding labels.\n",
        "These will be loaded directly by the model training notebook."
      ],
      "metadata": {
        "id": "fdnIDH3JaTtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all necessary variables are defined and not None/empty\n",
        "save_successful = True\n",
        "if 'X_train_final' in globals() and X_train_final is not None and \\\n",
        "   'y_train_final' in globals() and not y_train_final.empty and \\\n",
        "   'X_val_tfidf' in globals() and X_val_tfidf is not None and \\\n",
        "   'y_val' in globals() and not y_val.empty and \\\n",
        "   'X_test_tfidf' in globals() and X_test_tfidf is not None and \\\n",
        "   'y_test' in globals() and not y_test.empty:\n",
        "\n",
        "    try:\n",
        "        print(\"Saving TF-IDF features (sparse matrices)...\")\n",
        "        save_npz('X_train_features.npz', X_train_final)\n",
        "        save_npz('X_val_features.npz', X_val_tfidf)\n",
        "        save_npz('X_test_features.npz', X_test_tfidf)\n",
        "        print(\"TF-IDF features saved as .npz files in /content/\")\n",
        "\n",
        "        print(\"\\nSaving labels...\")\n",
        "        y_train_final.to_csv('y_train_labels.csv', index=False, header=['label'])\n",
        "        y_val.to_csv('y_val_labels.csv', index=False, header=['label'])\n",
        "        y_test.to_csv('y_test_labels.csv', index=False, header=['label'])\n",
        "        print(\"Labels saved as .csv files in /content/\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving features/labels: {e}\")\n",
        "        save_successful = False\n",
        "\n",
        "else:\n",
        "    print(\"One or more feature sets (X_train_final, X_val_tfidf, X_test_tfidf) or label sets (y_train_final, y_val, y_test) are missing or empty. Nothing saved.\")\n",
        "    save_successful = False\n",
        "\n",
        "if save_successful:\n",
        "    print(\"\\nFeature engineering complete. Output files are ready for model training.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nFeature engineering encountered issues. Not all files may have been saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hZbMJz_a9wx",
        "outputId": "8b83f07e-ea8c-4f1e-94a7-59c6bcbc3c76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TF-IDF features (sparse matrices)...\n",
            "TF-IDF features saved as .npz files in /content/\n",
            "\n",
            "Saving labels...\n",
            "Labels saved as .csv files in /content/\n",
            "\n",
            "Feature engineering complete. Output files are ready for model training.\n"
          ]
        }
      ]
    }
  ]
}